{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab68dd09-2c1d-4efb-b498-7a71a45aad96",
   "metadata": {},
   "source": [
    "# Merge of OpenAlex & Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d179a-9b8c-4bf2-87ac-4cfa7e47053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    \n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "    \n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\;\\:\\!\\?\\(\\)\\[\\]\\{\\}\\-\\'\\\"\\&\\%\\$\\#\\@\\+\\=\\/\\\\]', '', text)\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def normalize_date(date_str):\n",
    "    if pd.isna(date_str) or not date_str:\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        formats = [\n",
    "            '%Y-%m-%d', '%Y/%m/%d', '%d-%m-%Y', '%d/%m/%Y',\n",
    "            '%Y-%m', '%Y/%m', '%m-%Y', '%m/%Y',\n",
    "            '%Y'\n",
    "        ]\n",
    "        \n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                dt = datetime.strptime(str(date_str), fmt)\n",
    "                if fmt == '%Y':\n",
    "                    return f\"{dt.year}-01-01\"  # Si seulement l'année est disponible\n",
    "                elif fmt in ['%Y-%m', '%Y/%m', '%m-%Y', '%m/%Y']:\n",
    "                    return f\"{dt.year}-{dt.month:02d}-01\"  # Si seulement l'année et le mois sont disponibles\n",
    "                else:\n",
    "                    return dt.strftime('%Y-%m-%d')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        year_match = re.search(r'(\\d{4})', str(date_str))\n",
    "        if year_match:\n",
    "            return f\"{year_match.group(1)}-01-01\"\n",
    "            \n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def merge_and_clean_data(arxiv_file, openalex_file, output_file=None):\n",
    "    print(f\"Chargement des données arXiv depuis {arxiv_file}...\")\n",
    "    arxiv_df = pd.read_csv(\"data/\"+arxiv_file)\n",
    "    \n",
    "    print(f\"Chargement des données OpenAlex depuis {openalex_file}...\")\n",
    "    openalex_df = pd.read_csv(\"data/\"+openalex_file)\n",
    "    \n",
    "    print(f\"arXiv: {len(arxiv_df)} articles, OpenAlex: {len(openalex_df)} articles\")\n",
    "    \n",
    "    arxiv_df['source'] = 'arxiv'\n",
    "    openalex_df['source'] = 'openalex'\n",
    "    \n",
    "    arxiv_columns = {\n",
    "        'id': 'source_id',\n",
    "        'title': 'title',\n",
    "        'authors': 'authors',\n",
    "        'abstract': 'abstract',\n",
    "        'published_date': 'publication_date',\n",
    "        'year': 'year',\n",
    "        'doi': 'doi',\n",
    "        'categories': 'categories'\n",
    "    }\n",
    "    \n",
    "    openalex_columns = {\n",
    "        'id': 'source_id',\n",
    "        'title': 'title',\n",
    "        'authors': 'authors', \n",
    "        'abstract': 'abstract',\n",
    "        'publication_date': 'publication_date',\n",
    "        'year': 'year',\n",
    "        'doi': 'doi',\n",
    "        'cited_by_count': 'cited_by_count',\n",
    "        'concepts': 'concepts',\n",
    "        'author_countries': 'countries',\n",
    "        'author_institutions': 'institutions',\n",
    "        'categories': 'categories'\n",
    "    }\n",
    "    \n",
    "    arxiv_df_renamed = arxiv_df.rename(columns={old: new for old, new in arxiv_columns.items() if old in arxiv_df.columns})\n",
    "    openalex_df_renamed = openalex_df.rename(columns={old: new for old, new in openalex_columns.items() if old in openalex_df.columns})\n",
    "    \n",
    "    arxiv_cols = list(arxiv_columns.values())\n",
    "    arxiv_cols.append('source')\n",
    "    arxiv_df_selected = arxiv_df_renamed[[col for col in arxiv_cols if col in arxiv_df_renamed.columns]]\n",
    "    \n",
    "    openalex_cols = list(openalex_columns.values())\n",
    "    openalex_cols.append('source')\n",
    "    openalex_df_selected = openalex_df_renamed[[col for col in openalex_cols if col in openalex_df_renamed.columns]]\n",
    "    \n",
    "    for col in set(arxiv_cols + openalex_cols):\n",
    "        if col not in arxiv_df_selected.columns:\n",
    "            arxiv_df_selected[col] = np.nan\n",
    "        if col not in openalex_df_selected.columns:\n",
    "            openalex_df_selected[col] = np.nan\n",
    "            \n",
    "    print(\"Fusion des datasets...\")\n",
    "    merged_df = pd.concat([arxiv_df_selected, openalex_df_selected], ignore_index=True)\n",
    "    \n",
    "    print(\"Nettoyage des données...\")\n",
    "    \n",
    "    merged_df['title'] = merged_df['title'].apply(clean_text)\n",
    "    merged_df['abstract'] = merged_df['abstract'].apply(clean_text)\n",
    "    \n",
    "    merged_df['publication_date'] = merged_df['publication_date'].apply(normalize_date)\n",
    "    \n",
    "    for idx, row in merged_df.iterrows():\n",
    "        if pd.isna(row['year']) and row['publication_date']:\n",
    "            try:\n",
    "                date_parts = row['publication_date'].split('-')\n",
    "                if len(date_parts) >= 1:\n",
    "                    merged_df.at[idx, 'year'] = int(date_parts[0])\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    print(\"Suppression des doublons...\")\n",
    "    merged_df['title_lower'] = merged_df['title'].str.lower()\n",
    "    merged_df = merged_df.sort_values('cited_by_count', ascending=False, na_position='last')\n",
    "    merged_df = merged_df.drop_duplicates(subset=['title_lower', 'year'], keep='first')\n",
    "    merged_df = merged_df.drop(columns=['title_lower'])\n",
    "    \n",
    "    merged_df['year'] = merged_df['year'].fillna(0).astype(int)\n",
    "    merged_df['cited_by_count'] = merged_df['cited_by_count'].fillna(0).astype(int)\n",
    "    \n",
    "    merged_df = merged_df[(merged_df['year'] >= 2019) & (merged_df['year'] <= 2024)]\n",
    "    \n",
    "    merged_df['id'] = [f\"paper_{i+1}\" for i in range(len(merged_df))]\n",
    "    \n",
    "    cols_order = ['id', 'source', 'source_id', 'title', 'authors', 'abstract', 'publication_date', \n",
    "                 'year', 'doi', 'categories', 'concepts', 'cited_by_count', \n",
    "                 'countries', 'institutions']\n",
    "    \n",
    "    final_cols = [col for col in cols_order if col in merged_df.columns]\n",
    "    merged_df = merged_df[final_cols]\n",
    "    \n",
    "    print(f\"Processus terminé. Dataset final: {len(merged_df)} articles uniques.\")\n",
    "    \n",
    "    if output_file is None:\n",
    "        output_file = f'data/merged_generative_ai_data_{datetime.now().strftime(\"%Y%m%d\")}.csv'\n",
    "        \n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f\"Données fusionnées et nettoyées sauvegardées dans {output_file}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    arxiv_files = [f for f in os.listdir('data') if f.startswith('arxiv_generative_ai_data') and f.endswith('.csv')]\n",
    "    openalex_files = [f for f in os.listdir('data') if f.startswith('openalex_generative_ai_data') and f.endswith('.csv')]\n",
    "    \n",
    "    if arxiv_files and openalex_files:\n",
    "        arxiv_file = sorted(arxiv_files)[-1]\n",
    "        openalex_file = sorted(openalex_files)[-1]\n",
    "        \n",
    "        print(f\"Utilisation des fichiers les plus récents:\")\n",
    "        print(f\"  arXiv: {arxiv_file}\")\n",
    "        print(f\"  OpenAlex: {openalex_file}\")\n",
    "        \n",
    "        merged_df = merge_and_clean_data(arxiv_file, openalex_file)\n",
    "        \n",
    "        print(\"\\nRésumé des données fusionnées:\")\n",
    "        print(f\"Nombre total d'articles: {len(merged_df)}\")\n",
    "        \n",
    "        years_count = merged_df['year'].value_counts().sort_index()\n",
    "        print(\"\\nDistribution par année:\")\n",
    "        for year, count in years_count.items():\n",
    "            print(f\"  {year}: {count} articles\")\n",
    "        \n",
    "        sources_count = merged_df['source'].value_counts()\n",
    "        print(\"\\nDistribution par source:\")\n",
    "        for source, count in sources_count.items():\n",
    "            print(f\"  {source}: {count} articles\")\n",
    "    else:\n",
    "        print(\"Aucun fichier de données trouvé. Veuillez d'abord exécuter les scripts de collecte de données.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7ed6b8-3892-4e4a-be40-da416361938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isnull().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
